{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Parallel MD Generation using Multiprocessing\n",
    "\n",
    "This notebook generates molecular dynamics trajectories (`.xyz` files) for graphene nanoribbons.\n",
    "It uses `multiprocessing` to run simulations for different parameters (Temperature, Length) in parallel, utilizing multiple CPU cores.\n",
    "\n",
    "**Note:** The core simulation logic has been moved to `md_worker.py` to ensure compatibility with Python's `multiprocessing` on Windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Import the worker function from the external file\n",
    "# This is required for multiprocessing on Windows\n",
    "try:\n",
    "    import md_worker\n",
    "    from md_worker import worker_wrapper\n",
    "except ImportError:\n",
    "    print(\"Error: md_worker.py not found. Please ensure it is in the same directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(5,50,10).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "\n",
    "# 1. Define Parameters\n",
    "temperatures = [300] # Kelvin\n",
    "lengths = np.linspace(5, 20, 16).astype(int) # Unit cells\n",
    "\n",
    "# 2. Simulation Steps\n",
    "nsteps = 1000       # Total MD steps\n",
    "dump_interval = 1   # Write to file every N steps\n",
    "\n",
    "# 3. Output Directory\n",
    "output_dir = \"MD_files\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 4. Prepare Task List\n",
    "# Format: (temp, length, filename, nsteps, interval)\n",
    "tasks = []\n",
    "\n",
    "print(\"Preparing simulation tasks:\")\n",
    "for T in temperatures:\n",
    "    for L in lengths:\n",
    "        filename = os.path.join(output_dir, f\"md_T{T}_L{L}.xyz\")\n",
    "        # We need absolute path for safety in workers\n",
    "        # abs_filename = os.path.abspath(filename) --- IGNORE ---\n",
    "        \n",
    "        # Tuple arguments for the worker_wrapper\n",
    "        # args: (temp_K, ntile_length, filename, nsteps, dump_interval)\n",
    "        task_args = (T, L, filename, nsteps, dump_interval)\n",
    "        tasks.append(task_args)\n",
    "        print(f\"  - T={T}K, L={L} -> {filename}\")\n",
    "\n",
    "print(f\"\\nTotal tasks: {len(tasks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXECUTION ---\n",
    "\n",
    "def run_parameter_sweep_parallel():\n",
    "    # Determine number of processes\n",
    "    # Leave one core free for OS responsiveness if possible\n",
    "    num_processes = max(1, multiprocessing.cpu_count() - 2)\n",
    "    print(f\"Starting pool with {num_processes} processes...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use context manager for safety\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        # map_async allows us to catch errors or track progress if implemented, \n",
    "        # but map is simpler and blocks until done.\n",
    "        results = pool.map(worker_wrapper, tasks)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nAll simulations completed in {duration:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # This check is good practice, though not strictly required inside the cell logic\n",
    "    # unless exporting to script.\n",
    "    completed_files = run_parameter_sweep_parallel()\n",
    "    \n",
    "    print(\"\\nGenerated Files:\")\n",
    "    for f in completed_files:\n",
    "        print(f\"  {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VERIFICATION ---\n",
    "from ase.io import read\n",
    "\n",
    "print(\"Verifying randomly selected file...\")\n",
    "if len(tasks) > 0:\n",
    "    test_file = tasks[0][2] # Get filename of first task\n",
    "    if os.path.exists(test_file):\n",
    "        try:\n",
    "            atoms = read(test_file, index=':')\n",
    "            print(f\"File: {test_file}\")\n",
    "            print(f\"Number of frames: {len(atoms)}\")\n",
    "            print(f\"Number of atoms: {len(atoms[0])}\")\n",
    "            print(\"Status: OK\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {test_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"File {test_file} was not created.\")\n",
    "else:\n",
    "    print(\"No tasks defined.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
